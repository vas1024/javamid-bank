
Инфраструктура

разработка на джаве велась на компе под виндовс, проект пушится в гитхаб,
на дугом компе под линукс делается только пулл из гитхаба.
на линукс компе установлен дженкинс, докер, кинд, хелм


порядок ручного деплоя

git pull
cd $SERVICE_NAME
mvn clean package -DskipTest
docker build -t $SERVICE_NAME:latest .
kind load docker-image $SERVICE_NAME:latest
helm upgrade --install $SERVICE_NAME ./chart
kubectl rollout restart deployment/$SERVICE_NAME  
или
kubectl rollout restart statefulset/$SERVICE_NAME



деплой из Дженкинс

в проекте есть глобальный Jenkinsfile
он деплоит сервисы, список которых берет из  .Values.global.fullListOfServices  
имя переменной старался сделать уникальным, потому что мой дженкинс груви не умеет читать ямлы 
и приходится просто грепать по длинному имени
так же сделал список тулзов, в которых пока только кафка

в скрипте каждый сервис деплоится отдельным helm install так, что его можно легко передеплоить
на практике при разработке я пользовался скриптом run.sh <service name> для деплоя сервиса, код которогя я менял.



Кафка

несколько дней пытался настроить двухнодовую кафку, добился того, что кафка стартует, можно писать сообщения в топик,
но почему-то не получается читать из топика. вернее, получается только, если указать партицию.
понял, что смысла в двухнодовой кафке неммного, так как для кворума необходимо более 50 процентов, таким образом
если одна нода лежит, вторая тоже не будет работать. 
сдался, сделал однонодовую кафку, она запускается в statufulset в который монтируется pvc /kafka/data
разочаровался в configmap так как для двухнодовой кафки было важно прописать в нее параметр зависящий от hostname, 
но конфигмапа неизменяемая. может быть есть какие-то хитрости, но мне пока удобнее генерировать конфиг файлы баш скриптом.



доступ из браузера к микросервису front

для доступа по http к приложению, надо после запуска всех сервисов сделать порт форвард,
в отдельной консоли запустить команду
kubectl port-forward service/front 8080:8080 --address 0.0.0.0
в браузере http://<host with kuber>:8080/login



логика работы блока наличных cash

счет account создается при внесении на него наличных денег.
счет не удаляется. 
если на счету 0, с него нельзя снять деньги, будет ошибка.
если счета нет, с него нельзя снять деньги, будет ошибка.
при попытке снять больше денег, чем есть на счету, выдается всё, что есть и баланс счета становится 0.
это удобно для тестов на удаление пользователя.


логика работы сервисов exchange - exgen

сервис exchange является хранителем курсов,а так же списка доступных в приложении валют.
сервис exgen считывает список валют и текущие курсы стандартным гет запросом,
генерирует новые и отсылает в топик кафки rates.
на примере взаимодействия этих двух сервисов я делал oauth2 авторизацию, 
заморочился с exchange так чтобы он отдавал курсы без проверок всем желающим, 
но менять курсы можно только с авторизацией. у меня рука не поднимается всё это вырезать



безопасность

безопасность пользователей

логика работы
при введении логина-пароля в форме, мс front оправляет запрос с username, password  в мс auth
мс auth передает этот запрос в мс accounts, где происходит сравнение с сохраненным хешем пароля, и если ок, 
то мс accounts возвращает в мс auth userId, -1 в случае ошибки
мс auth в случае успешного ответа из accounts генерирует JWT token в который добавляет username и userId
мс front сохраняет полученный токен в куки jwt_token.
в мс front настроен SecurityConfig:
сначала срабатывает jwtAuthenticationFilter, который отправляет в мс auth запрос на валидацию токена,
а затем срабатывает authorizeHttpRequests так, чтобы при обращении по путям, содержащим id пользователя,
вызывалась бы проверка, метод authorizeHttpRequests, 
который сравнивает запрашиваемый в пути userId с userId, хранящемся в токене

в сервисе accounts так же поднят SecurityConfig, который делает те же проверки - сравнивает userId в токене
c userId в запрашиваемом пути. для этого сервис front отправляет токен в заголовке ( требует использования restTemplate.exchange)
так же для запросов которые идут по цепочке front -> cash -> accounts  and  front->transfer->accounts 
сервисы cash и accounts читают токен пришедший от фронта и передают его без проверок в accounts

безопасность микросервисов

для демонстрации авторизации сервисов через OAuth2 были выбраны 2 сервиса exchange и exgen.
сервис exchange имеет два эндпоинта, один для чтения курсов валют, он незащищен, второй для записи, 
он защищен, нужен токен и проверяется scope=write.
для этого на сервисе auth был сделан самописный OAuth2 сервер. я там кстати так и не понял, правда ли, что надо самому
вручную создавать таблицы, в которые потом Спринг сам пишет. просто спринг сам всё автоматически делает, даже репозиторий 
сам создает, но вот уговорить его автосоздать таблицы не получилось.





Dynamic Discovery

для поиска мс исользуется docker dns, для этого у каждого микро сервиса создается 
кубер сущность сервис с именем как у сервиса, порт у всех мс 8080.
при нескольких инстансах одного МС load balancing осуществляется кубером,
для statefulset допускается только один инстанс и дипсик рекомендует обращаться по имени пода 
http://auth-0.auth:8080/api/users/123 чтобы не было путаницы если случайно запустить больше инстансов,
но будет работать и как для деплоя  http://auth:8080/api/users/123



базы данных

имеются у auth, accounts, notify
это H2 базы в файле
имиджи этих сервисов устанавливаются в кубер не как деплой, а как statefulset
базы данных в смонтированной папке /data
таким образом, данные МС нельзя масштабировать, иначе у каждого инстанса будет своя БД,
но по заданию можно обоходитья одиним инстансом ))
полезные команды
# Подключись к POD'у
kubectl exec -it auth-0 -- sh
# Внутри контейнера посмотри файлы БД
ls -la /data
# Скопируй файл БД из POD'а на локальную машину
kubectl  cp auth-0:/data .



helm tests

есть тест в зонтичном чарте, который дергает /actuator/health у каждого сервиса
запустить можно
helm test bank
посмотреть результат
kubectl logs test-bank-actuators


недоделки










