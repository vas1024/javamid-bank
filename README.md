
Инфраструктура

разработка на джаве велась на компе под виндовс, проект пушится в гитхаб,
на дугом компе под линукс делается только пулл из гитхаба.
на линукс компе установлен дженкинс, докер, кинд, хелм


проброс портов

kubectl port-forward service/front 8080:8080 --address 0.0.0.0   # это само приложение Банк
kubectl port-forward service/zipkin 9411:9411 --address 0.0.0.0
kubectl port-forward svc/prometheus-kube-prometheus-prometheus 9092:9090 --address 0.0.0.0
kubectl port-forward svc/prometheus-grafana 3000:80 --address 0.0.0.0
kubectl port-forward svc/prometheus-kube-prometheus-alertmanager 9093:9093 --address 0.0.0.0
kubectl port-forward pod/logstash-logstash-0 5044:5044
kubectl port-forward svc/elasticsearch-master 9200:9200 --address 0.0.0.0
kubectl port-forward svc/kibana 5601:5601 --address 0.0.0.0


zipkin 
рисует красивую схему взаимодействия между сервисами
мне не удалось найти трейсы от записи в бд
судя по схеме взаимодействий, нет трейса от записи в кафка, но есть трейс от чтения из кафки



prometheus, grafana
пароль для входа в графану admin/admin

у графаны при установке включается сайдкар для дашбордов, который позволяет графане подхватывать дашборды из конфигмап 
с правильным лейблом. так загружается написанная мною конфигмапа с требуемыми в задании графигами.

дашборды для JVM и http я предлагаю загружать руками по номерам
4701  jvm
11378 http

созданы два алерта  
один краснеет когда перестают обновляться exchange rates, 
другой - когда пользователь вводит больше трех неправильных пароля в течении минуты

для алерта с курсами валют таймер не подошел, так как он обновляется только когда после долгого перерыва приходит обновление курсов,
то есть, когда сервис снова поднят. он удобен для графиков, чтобы видеть когда были перерывы.
для алерта был использован гаудж, который показывает количество секунд с последнего обновления. 
гаудж вычисляется в момент, когда прометеус приходит забирать метрики.
курсы валют обновляются раз в 10 секунд, прометеус скрейпит данные раз в 15 секунд.
таким образом, при нормальной работе микросервисов гаудж показывает случайное число 0..10 ( на самом деле периодически 2 постоянных числа )
когда значение гауджа > 10, это означает пропуск в обновлении курса валют ( в проекте для надежности 15 секунд )
команда для остановки сервиса 
kubectl scale deployment exgen --replicas=0


ELK, log4j2
возникли трудности с установкой из еластик репозитория,
придумал такой путь
  docker pull образ с докерхаба
  kind load загрузка образа в кинд
  git clone https://github.com/elastic/helm-charts.git
  helm install logstash ./helm-charts/logstash -f ./logstash-values.yaml
но получилось это только с логстешем, красиво как в курсе, с вольюс файлом.  
долго бился чтобы запустить официальным чартом еластик на одной ноде, и так и не смог.
кибана вообще не хотела не только ставиться но даже и удаляться.
для установки еластика и кибаны самописный стс и деплой с сервисами.

в конфиге log4j2 захардкодено имя микросервиса, у меня основная переменная с именем service.name из application.properties
но она не может быть применена, так как log4j2.xml читается до application.properties при старте приложения

для правильного логирования мешался logback, зависимость с ним тянется с spring-boot-starter-web
по этому пришлось добавить exclusion spring-boot-starter-logging  в пом

для передачи логов используется топик кафка по имени logs




порядок ручного деплоя приложения

git pull
cd $SERVICE_NAME
mvn clean package -DskipTest
docker build -t $SERVICE_NAME:latest .
kind load docker-image $SERVICE_NAME:latest
helm upgrade --install $SERVICE_NAME ./chart
kubectl rollout restart deployment/$SERVICE_NAME  
или
kubectl rollout restart statefulset/$SERVICE_NAME



деплой из Дженкинс

в проекте есть глобальный Jenkinsfile
он деплоит сервисы, список которых берет из  .Values.global.fullListOfServices  
имя переменной старался сделать уникальным, потому что мой дженкинс груви не умеет читать ямлы 
и приходится просто грепать по длинному имени
так же сделал список тулзов, в которых пока только кафка

в скрипте каждый сервис деплоится отдельным helm install так, что его можно легко передеплоить
на практике при разработке я пользовался скриптом run.sh <service name> для деплоя сервиса, код которогя я менял.



Кафка

несколько дней пытался настроить двухнодовую кафку, добился того, что кафка стартует, можно писать сообщения в топик,
но почему-то не получается читать из топика. вернее, получается только, если указать партицию.
понял, что смысла в двухнодовой кафке неммного, так как для кворума необходимо более 50 процентов, таким образом
если одна нода лежит, вторая тоже не будет работать. 
сдался, сделал однонодовую кафку, она запускается в statufulset в который монтируется pvc /kafka/data
разочаровался в configmap так как для двухнодовой кафки было важно прописать в нее параметр зависящий от hostname, 
но конфигмапа неизменяемая. может быть есть какие-то хитрости, но мне пока удобнее генерировать конфиг файлы баш скриптом.



доступ из браузера к микросервису front

для доступа по http к приложению, надо после запуска всех сервисов сделать порт форвард,
в отдельной консоли запустить команду
kubectl port-forward service/front 8080:8080 --address 0.0.0.0
в браузере http://<host with kuber>:8080/login



логика работы блока наличных cash

счет account создается при внесении на него наличных денег.
счет не удаляется. 
если на счету 0, с него нельзя снять деньги, будет ошибка.
если счета нет, с него нельзя снять деньги, будет ошибка.
при попытке снять больше денег, чем есть на счету, выдается всё, что есть и баланс счета становится 0.
это удобно для тестов на удаление пользователя.


логика работы сервисов exchange - exgen

сервис exchange является хранителем курсов,а так же списка доступных в приложении валют.
сервис exgen считывает список валют и текущие курсы стандартным гет запросом,
генерирует новые и отсылает в топик кафки rates.
на примере взаимодействия этих двух сервисов я делал oauth2 авторизацию, 
заморочился с exchange так чтобы он отдавал курсы без проверок всем желающим, 
но менять курсы можно только с авторизацией. у меня рука не поднимается всё это вырезать



безопасность

безопасность пользователей

логика работы
при введении логина-пароля в форме, мс front оправляет запрос с username, password  в мс auth
мс auth передает этот запрос в мс accounts, где происходит сравнение с сохраненным хешем пароля, и если ок, 
то мс accounts возвращает в мс auth userId, -1 в случае ошибки
мс auth в случае успешного ответа из accounts генерирует JWT token в который добавляет username и userId
мс front сохраняет полученный токен в куки jwt_token.
в мс front настроен SecurityConfig:
сначала срабатывает jwtAuthenticationFilter, который отправляет в мс auth запрос на валидацию токена,
а затем срабатывает authorizeHttpRequests так, чтобы при обращении по путям, содержащим id пользователя,
вызывалась бы проверка, метод authorizeHttpRequests, 
который сравнивает запрашиваемый в пути userId с userId, хранящемся в токене

в сервисе accounts так же поднят SecurityConfig, который делает те же проверки - сравнивает userId в токене
c userId в запрашиваемом пути. для этого сервис front отправляет токен в заголовке ( требует использования restTemplate.exchange)
так же для запросов которые идут по цепочке front -> cash -> accounts  and  front->transfer->accounts 
сервисы cash и accounts читают токен пришедший от фронта и передают его без проверок в accounts

безопасность микросервисов

для демонстрации авторизации сервисов через OAuth2 были выбраны 2 сервиса exchange и exgen.
сервис exchange имеет два эндпоинта, один для чтения курсов валют, он незащищен, второй для записи, 
он защищен, нужен токен и проверяется scope=write.
для этого на сервисе auth был сделан самописный OAuth2 сервер. я там кстати так и не понял, правда ли, что надо самому
вручную создавать таблицы, в которые потом Спринг сам пишет. просто спринг сам всё автоматически делает, даже репозиторий 
сам создает, но вот уговорить его автосоздать таблицы не получилось.





Dynamic Discovery

для поиска мс исользуется docker dns, для этого у каждого микро сервиса создается 
кубер сущность сервис с именем как у сервиса, порт у всех мс 8080.
при нескольких инстансах одного МС load balancing осуществляется кубером,
для statefulset допускается только один инстанс и дипсик рекомендует обращаться по имени пода 
http://auth-0.auth:8080/api/users/123 чтобы не было путаницы если случайно запустить больше инстансов,
но будет работать и как для деплоя  http://auth:8080/api/users/123



базы данных

имеются у auth, accounts, notify
это H2 базы в файле
имиджи этих сервисов устанавливаются в кубер не как деплой, а как statefulset
базы данных в смонтированной папке /data
таким образом, данные МС нельзя масштабировать, иначе у каждого инстанса будет своя БД,
но по заданию можно обоходитья одиним инстансом ))
полезные команды
# Подключись к POD'у
kubectl exec -it auth-0 -- sh
# Внутри контейнера посмотри файлы БД
ls -la /data
# Скопируй файл БД из POD'а на локальную машину
kubectl  cp auth-0:/data .



helm tests

есть тест в зонтичном чарте, который дергает /actuator/health у каждого сервиса
запустить можно
helm test bank
посмотреть результат
kubectl logs test-bank-actuators


недоделки

zipkin - нет трейсов от обращений к бд, нет трейсов от записи в кафка ( от чтения из кафки есть )









