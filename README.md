
Инфраструктура

разработка на джаве велась на компе под виндовс, проект пушится в гитхаб,
на дугом компе под линукс делается только пулл из гитхаба.
на линукс компе установлен дженкинс, докер, кинд


порядок ручного деплоя

git pull
cd $SERVICE_NAME
mvn clean package -DskipTest
docker build -t $SERVICE_NAME:latest .
kind load docker-image $SERVICE_NAME:latest
helm upgrade --install $SERVICE_NAME ./chart
kubectl rollout restart deployment/$SERVICE_NAME  
или
kubectl rollout restart statefulset/$SERVICE_NAME



доступ из браузера к микросервису front

для доступа по http к приложению, надо после запуска всех сервисов сделать порт форвард,
в отдельной консоли запустить команду
kubectl port-forward service/front 8080:8080 --address 0.0.0.0
в браузере http://<host with kuber>:8080/login



логика работы блока наличных cash

счет account создается при внесении на него наличных денег.
счет не удаляется. 
если на счету 0, с него нельзя снять деньги, будет ошибка.
если счета нет, с него нельзя снять деньги, будет ошибка.
при попытке снять больше денег, чем есть на счету, выдается всё, что есть и баланс счета становится 0.
это удобно для тестов на удаление пользователя.




безопасность

безопасность пользователей

логика работы
при введении логина-пароля в форме, мс front оправляет запрос с username, password  в мс auth
мс auth передает этот запрос в мс accounts, где происходит сравнение с сохраненным хешем пароля, и если ок, 
то мс accounts возвращает в мс auth userId, -1 в случае ошибки
мс auth в случае успешного ответа из accounts генерирует JWT token в который добавляет username и userId
мс front сохраняет полученный токен в куки jwt_token.
в мс front настроен SecurityConfig:
сначала срабатывает jwtAuthenticationFilter, который отправляет в мс auth запрос на валидацию токена,
а затем срабатывает authorizeHttpRequests так, чтобы при обращении по путям, содержащим id пользователя,
вызывалась бы проверка, метод authorizeHttpRequests, 
который сравнивает запрашиваемый в пути userId с userId, хранящемся в токене

в сервисе accounts так же поднят SecurityConfig, который делает те же проверки - сравнивает userId в токене
c userId в запрашиваемом пути. для этого сервис front отправляет токен в заголовке ( требует использования restTemplate.exchange)
так же для запросов которые идут по цепочке front -> cash -> accounts  and  front->transfer->accounts 
сервисы cash и accounts читают токен пришедший от фронта и передают его без проверок в accounts

безопасность микросервисов

для демонстрации авторизации сервисов через OAuth2 были выбраны 2 сервиса exchange и exgen.
сервис exchange имеет два эндпоинта, один для чтения курсов валют, он незащищен, второй для записи, 
он защищен, нужен токен и проверяется scope=write.
для этого на сервисе auth был сделан самописный OAuth2 сервер. я там кстати так и не понял, правда ли, что надо самому
вручную создавать таблицы, в которые потом Спринг сам пишет. просто спринг сам всё автоматически делает, даже репозиторий 
сам создает, но вот уговорить его автосоздать таблицы не получилось.





Dynamic Discovery

для поиска мс исользуется docker dns, для этого у каждого микро сервиса создается 
кубер сущность сервис с именем как у сервиса, порт у всех мс 8080.
при нескольких инстансах одного МС load balancing осуществляется кубером,
для statefulset допускается только один инстанс и дипсик рекомендует обращаться по имени пода 
http://auth-0.auth:8080/api/users/123 чтобы не было путаницы если случайно запустить больше инстансов,
но будет работать и как для деплоя  http://auth:8080/api/users/123



базы данных

имеются у auth, accounts, notify
это H2 базы в файле
имиджи этих сервисов устанавливаются в кубер не как деплой, а как statefulset
базы данных в смонтированной папке /data
таким образом, данные МС нельзя масштабировать, иначе у каждого инстанса будет своя БД,
но по заданию можно обоходитья одиним инстансом ))
полезные команды
# Подключись к POD'у
kubectl exec -it auth-0 -- sh
# Внутри контейнера посмотри файлы БД
ls -la /data
# Скопируй файл БД из POD'а на локальную машину
kubectl  cp auth-0:/data .




helm tests

есть тест в зонтичном чарте, который дергает /actuator/health у каждого сервиса
запустить можно
helm test bank
посмотреть результат
kubectl logs test-bank-actuators



недоделки

- разные среды ( dev, test, prod ) в разных namespace. как-то неразумно, ведь будетвзаимовлияние по производительности,
уж лушче в разных кластерах.
- не понял смысл зонтичного чарта. если задеплоить все сервисы приложения bank одним чартом, а затем попробовать
передеплоить один сервис своим чартом, возникает конфликт имен ресурсов (конфигмапы, секреты) такие ресурсы уже есть 
и хелм помнит, кто ( какой чарт ) их создал и не дает удалять. можно удалить кубером, тогда у нас будет поломанный зонтичны чарт.
неудобно




ревью к 9му спринту

- ты в репозиторий закоммитил файлы базы (*.db, *.lock.db). Мы обычно на проекте так не делаем, добавь их  в .gitignore`.
- В cash и transfer контроллерах ты вручную прокидываешь токен из запроса в следующий вызов. Это будет повторяться везде. 
Лучше настрой RestTemplate с ClientHttpRequestInterceptor, который будет автоматом Authorization копировать.
- И вот это DB_PATH:~/IdeaProjects/javamid-bank/data в конфиге, так не надо, путь должен быть относительным 
или полностью управляться через переменные окружения в Docker.
- Ну и пиши тесты :)
Александр Передерей
ревьюер



